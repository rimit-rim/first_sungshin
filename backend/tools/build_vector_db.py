from pathlib import Path
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
import os
import sys

# ê²½ë¡œ/í™˜ê²½ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parents[1]  # backend/
ENV_PATH = BASE_DIR / ".env"
load_dotenv(ENV_PATH)  # ë£¨íŠ¸ .env ë¡œë“œ

API_KEY = os.getenv("OPENAI_API_KEY", "")
if not API_KEY:
    print("OPENAI_API_KEYê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•˜ì„¸ìš”:", ENV_PATH)
    sys.exit(1)

DATA_DIR = BASE_DIR / "data"
PDF_KO = DATA_DIR / "sungshin_info_kor.pdf"
PDF_EN = DATA_DIR / "sungshin_info_eng.pdf"
VEC_DIR = DATA_DIR / "vector"
VEC_DIR.mkdir(parents=True, exist_ok=True)

# ìœ í‹¸
def to_chunks(page_docs, size=800, overlap=100):
    full_text = "\n\n".join([doc.page_content for doc in page_docs])
    splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)
    return splitter.create_documents([full_text])

def build_one(pdf_path: Path, out_name: str):
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF not found: {pdf_path}")

    print(f"ğŸ“„ Loading: {pdf_path}")
    loader = PyPDFLoader(str(pdf_path))
    pages = loader.load()
    chunks = to_chunks(pages)

    print(f"ğŸ”§ Embedding & building FAISS ({out_name})...")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=API_KEY)
    vs = FAISS.from_documents(chunks, embeddings)

    out_dir = VEC_DIR / out_name
    vs.save_local(str(out_dir))
    print(f"âœ… Saved: {out_dir}")

def main():
    print(f"ğŸ BASE_DIR: {BASE_DIR}")
    print(f"ğŸ“‚ DATA_DIR: {DATA_DIR}")
    build_one(PDF_KO, "faiss_kor")
    build_one(PDF_EN, "faiss_eng")
    print("ğŸ‰ All vector DBs built successfully.")

if __name__ == "__main__":
    main()

## ì½”ë“œ ì‹¤í–‰ : PDF ì¶”ê°€, êµì²´ì‹œë§Œ ì‹¤í–‰
# python tools/build_vector_db.py